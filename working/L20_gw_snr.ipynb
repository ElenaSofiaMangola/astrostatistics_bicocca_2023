{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a27eee24-a8b0-4b74-b097-67f0d761e871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elenasofiamangola/.local/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_predict, GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, f1_score, accuracy_score, precision_score, recall_score\n",
    "from tabulate        import tabulate\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3dcbd9-cb35-41ed-ab33-f2e5ca4f1dc1",
   "metadata": {},
   "source": [
    "**Can a computer learn if we're going to detect gravitational waves?**\n",
    "All experiments have selection effects. Some sources are easier to detect than others which distorts the population of sources we observe (this crucial in astronomy! Surveys are typically flux limited). In order to decided if a feature in the observed population of objects is telling us something new about reality, we need to understand and model our selection effects (for instance: it would be wrong to say that all stars are close by just because we can't observe those that are very far!). \n",
    " \n",
    "**The goal here is to machine-learn the LIGO detectability: can we *predict* if a gravitational-wave source will be detected?**  \n",
    "\n",
    "[This dataset](https://github.com/dgerosa/pdetclassifier/releases/download/v0.2/sample_2e7_design_precessing_higherordermodes_3detectors.h5) contains simulated gravitational-wave signals from merging black holes (careful the file size is >1 GB). You can read this data format with `h5py`.\n",
    "\n",
    "In particular, each source has the following features:\n",
    "- `mtot`: the total mass of the binary\n",
    "- `q`: the mass ratio\n",
    "- `chi1x`, `chi1y`, `chi1z`, `chi2x`, `chi2y`, `chi2z`: the components of the black-hole spins in a suitable reference frame.\n",
    "- `ra`, `dec`: the location of the source in the sky\n",
    "- `iota`: the inclination of the orbital plane'\n",
    "- `psi`: the polarization angle (gravitational waves have two polarization states much like light)\n",
    "- `z`: the redshift\n",
    "\n",
    "The detectability is defined using the `snr` (signal-to-noise ratio) computed with a state-of-the-art model of the LIGO/Virgo detector network. Some (many?) of you will have studied this in Sesana's gravitational-wave course; [see here](https://arxiv.org/abs/1908.11170) for a nice write-up. All you need to know now is that we threshold the `snr` values and assume that LIGO will (not) see a source if `snr`>12 (`snr`<12). The resulting 0-1 labels are reported in the `det` attribute in the dataset.\n",
    "\n",
    "Today's task is to train a classifier (you decide which one!) and separate sources that are detectables from those that aren't. \n",
    "\n",
    "Be creative! This is a challenge! Let's see who gets the  higher completeness and/or the smaller contamination (on a validation set, of course! Careful with overfitting here!).\n",
    "\n",
    "*Tips*:\n",
    "- You can downsample the data for debugging purposes\n",
    "- You can also use only some of the features. By experience, the most important ones are those involving masses and redshift.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04ed4072-f13e-4781-9150-069da14d278c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chi1x',\n",
       " 'chi1y',\n",
       " 'chi1z',\n",
       " 'chi2x',\n",
       " 'chi2y',\n",
       " 'chi2z',\n",
       " 'dec',\n",
       " 'det',\n",
       " 'iota',\n",
       " 'mtot',\n",
       " 'psi',\n",
       " 'q',\n",
       " 'ra',\n",
       " 'snr',\n",
       " 'z']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = h5py.File('sample_2e7_design_precessing_higherordermodes_3detectors.h5', 'r')\n",
    "keys=f.keys()\n",
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "712c84d7-98a2-459e-bb63-0895e8631b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# components\n",
    "chi1x = f['chi1x'][::1000]\n",
    "chi1y = f['chi1y'][::1000]\n",
    "chi1z = f['chi1z'][::1000]\n",
    "chi2x = f['chi2x'][::1000]\n",
    "chi2y = f['chi2x'][::1000]\n",
    "chi2z = f['chi2x'][::1000]\n",
    "dec = f['dec'][::1000]\n",
    "iota = f['iota'][::1000]\n",
    "mtot = f['mtot'][::1000]\n",
    "psi = f['psi'][::1000]\n",
    "q = f['q'][::1000]\n",
    "ra = f['ra'][::1000]\n",
    "z = f['z'][::1000]\n",
    "# label\n",
    "snr = f['snr'][::1000]\n",
    "det = f['det'][::1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddc73a67-37c9-403d-9355-489933963d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = np.array([chi1x,chi1y,chi1z,chi2x,chi2y,chi2z,dec,iota,mtot,psi,q,ra,z]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "711ec860-6bc9-4574-a2af-40b37a3036e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(components, det, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c411a903-767d-496f-a5ce-6b75e17c83a7",
   "metadata": {},
   "source": [
    "First of all, I want to try using all the components. <br>\n",
    "Let's try with a **Random Forest**. <br>\n",
    "To determine which is the best depth I use cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889e0108-ccab-461d-88b5-5f899febf0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf    = KFold(n_splits=3, shuffle=True)\n",
    "# depth = how many split each decision tree can have\n",
    "depth = np.linspace(1, 30, 30, dtype=int)\n",
    "# n_estimators = number of trees\n",
    "model_rf = RandomForestClassifier(n_estimators=100)\n",
    "grid     = GridSearchCV(model_rf, param_grid={'max_depth': depth}, cv=kf)\n",
    "grid.fit(xtrain, ytrain)\n",
    "best_rf  = grid.best_params_['max_depth']\n",
    "print(best_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e00493-eccf-41ea-909c-bf999188f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf_best = RandomForestClassifier(n_estimators=100, max_depth=best_rf)\n",
    "model_rf_best.fit(xtrain, ytrain)\n",
    "ypred = model_rf_best.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd87833-718c-4ee5-b622-bddb60222853",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy  = accuracy_score(ytest, ypred)\n",
    "precision = precision_score(ytest, ypred)\n",
    "recall    = recall_score(ytest, ypred)\n",
    "print('Accuracy = ', accuracy)\n",
    "print('Precision (purity) = ', precision)\n",
    "print('Completeness (recall)= ', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec5975d-9f04-4feb-9807-768059759403",
   "metadata": {},
   "source": [
    "What I did right now was to use all the components, but maybe I don't need all of them. <br>\n",
    "I run a PCA reduction to see what are the major components that have the most variance of the data (with scree plots), and I will then only use them to see if something change in the Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec4505c-65d1-4d5a-8f1f-9273ba6be17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(components[0])\n",
    "pca = PCA(n_components = n_features, whiten=True)\n",
    "# PCS is sensitive to the scale of features, so I normalize them\n",
    "scaled_components = scaler.fit_transform(components)\n",
    "pca.fit(scaled_components) \n",
    "evals = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea57cf4-f613-4183-aa56-3981d3cadd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3))\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "\n",
    "ax.plot(np.arange(n_features), evals)\n",
    "ax.scatter(np.arange(n_features), evals)\n",
    "\n",
    "ax.set_xlabel('eigenvalue')\n",
    "ax.set_ylabel('explained variance ratio')\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "\n",
    "ax.plot(np.arange(n_features), evals.cumsum())\n",
    "ax.scatter(np.arange(n_features), evals.cumsum())\n",
    "\n",
    "ax.set_xlabel('eigenvalue number')\n",
    "ax.set_ylabel('cumulative explained variance ratio')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c997c5-92c0-4f39-aa7a-983fc69b34ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ce524b-1ccb-47cc-9c69-2da7f36ed4be",
   "metadata": {},
   "source": [
    "Basically, all the dataset can be explained using only one principal component!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb45d5-1626-401c-aa49-da395bd9bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the weight matrix that tells me the relationship betewwn each principal components and my physical components\n",
    "# RB each PC is a linear combination of the physical components\n",
    "loadings  = pca.components_\n",
    "first_PC  = loadings[0:4]\n",
    "row_names = ['chi1x','chi1y','chi1z','chi2x','chi2y','chi2z','dec','iota','mtot','psi','q','ra','z']\n",
    "tabella_ = [[row_names[i], first_PC[0][i], first_PC[1][i], first_PC[2][i], first_PC[3][i]] for i in range(len(row_names))]\n",
    "headers   = ['Feature', 'PC1', 'PC2', 'PC3', 'PC4']\n",
    "tabulate(tabella_, headers=headers, tablefmt='html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76181d3a-9e85-4a85-8661-a429ba5cf675",
   "metadata": {},
   "source": [
    "Now, I can try to do classification using only the principal components that gave me at leat 95% of variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d517a6e5-f076-467b-9783-f88f2f7b898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want to retrain only one principal component\n",
    "pca = PCA(n_components = 0.95, whiten=True)\n",
    "pca.fit(scaled_components)\n",
    "# I project my training dataset on the principal component axis\n",
    "scaler = StandardScaler()\n",
    "x_scaled_train = scaler.fit_transform(xtrain)\n",
    "x_scaled_test  = scaler.transform(xtest)\n",
    "x_pca_train  = pca.transform(x_scaled_train)\n",
    "x_pca_test   = pca.transform(x_scaled_test)\n",
    "# now I train again the random forest\n",
    "model_rf_pca = RandomForestClassifier(n_estimators=100)\n",
    "model_rf_pca.fit(x_pca_train, ytrain) \n",
    "ypred_pca = model_rf_pca.predict(x_pca_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f126380-c868-4035-ab1b-6631b14af471",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_pca  = accuracy_score(ytest, ypred_pca)\n",
    "precision_pca = precision_score(ytest, ypred_pca)\n",
    "recall_pca    = recall_score(ytest, ypred_pca)\n",
    "print('Accuracy = ', accuracy_pca)\n",
    "print('Precision (purity) = ', precision_pca)\n",
    "print('Completeness (recall)= ', recall_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0ed123-23d4-4610-b249-b0ad46944f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_  = np.array([[accuracy, precision, recall],[accuracy_pca, precision_pca, recall_pca]])\n",
    "col_names = ['accuracy', 'precision', 'completeness']\n",
    "row_names = ['all features', '95% var - PCA']\n",
    "tabella_  = [[col] + list(row) for col, row in zip(row_names, metrics_)]\n",
    "headers   = [''] + col_names\n",
    "tabulate(tabella_, headers=headers, tablefmt='html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddead5e-2798-41db-8106-edea6ec12b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
